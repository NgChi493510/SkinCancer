{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1853224,"sourceType":"datasetVersion","datasetId":1102351},{"sourceId":10709796,"sourceType":"datasetVersion","datasetId":6631190},{"sourceId":10846341,"sourceType":"datasetVersion","datasetId":6621869},{"sourceId":268961,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":230160,"modelId":251905}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torch\nimport pandas as pd\nfrom torchvision import transforms\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import random_split, DataLoader\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:53:02.959522Z","iopub.execute_input":"2025-02-26T09:53:02.959904Z","iopub.status.idle":"2025-02-26T09:53:02.964475Z","shell.execute_reply.started":"2025-02-26T09:53:02.959876Z","shell.execute_reply":"2025-02-26T09:53:02.963654Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None, is_test=False):\n        self.data = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        self.is_test = is_test\n        self.class_mapping = {'AK': 0, 'BCC': 1, 'BKL': 2, 'DF': 3, 'MEL': 4, \n                              'NV': 5, 'SCC': 6, 'VASC': 7}\n\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if not self.is_test:\n            \n            img_path = os.path.join(self.root_dir, self.data.iloc[idx, 1])\n            image = Image.open(img_path).convert('RGB')\n            class_name = self.data.iloc[idx, 0]  \n            label = self.class_mapping.get(class_name, -1)  \n            label = torch.tensor(label, dtype=torch.long)\n\n            if self.transform:\n                image = self.transform(image)\n\n            return image, label\n        else:\n            # For test data\n            img_path = os.path.join(self.root_dir, self.data.iloc[idx, 4])\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image, self.data.iloc[idx, 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:53:05.019088Z","iopub.execute_input":"2025-02-26T09:53:05.019384Z","iopub.status.idle":"2025-02-26T09:53:05.027012Z","shell.execute_reply.started":"2025-02-26T09:53:05.019360Z","shell.execute_reply":"2025-02-26T09:53:05.025954Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"image_size = 224\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),  # ⚠️ Di chuyển ToTensor() lên trước\n    transforms.Normalize(mean, std),\n    transforms.GaussianBlur(kernel_size=3),\n    transforms.RandomErasing(p=0.7)\n])\n\nvalid_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:53:07.593960Z","iopub.execute_input":"2025-02-26T09:53:07.594238Z","iopub.status.idle":"2025-02-26T09:53:07.600643Z","shell.execute_reply.started":"2025-02-26T09:53:07.594220Z","shell.execute_reply":"2025-02-26T09:53:07.599652Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_dataset = ImageDataset(\n    csv_file='/kaggle/input/skincancer-isic2019/image_paths.csv',  \n    root_dir='/kaggle/input/skincancer-isic2019/archive/archive',\n    transform=train_transform\n)\n\ntest_dataset = ImageDataset(\n    csv_file='/kaggle/input/model-datatest-skincancer/test_data_remove_unk.csv', \n    root_dir='/kaggle/input/isic-2019-challenge/ISIC_2019_Test_Input/ISIC_2019_Test_Input', \n    transform=valid_transform, is_test=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:53:09.936271Z","iopub.execute_input":"2025-02-26T09:53:09.936552Z","iopub.status.idle":"2025-02-26T09:53:10.056704Z","shell.execute_reply.started":"2025-02-26T09:53:09.936532Z","shell.execute_reply":"2025-02-26T09:53:10.056016Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_size = int(0.8 * len(train_dataset))\nvalid_size = len(train_dataset) - train_size\n\ntrainset, validset = random_split(train_dataset, [train_size, valid_size])\n\n\nbatch_size = 32\ntrain_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(validset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:53:13.019074Z","iopub.execute_input":"2025-02-26T09:53:13.019389Z","iopub.status.idle":"2025-02-26T09:53:13.076167Z","shell.execute_reply.started":"2025-02-26T09:53:13.019368Z","shell.execute_reply":"2025-02-26T09:53:13.075486Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class CustomEfficientNet(nn.Module):\n    def __init__(self, num_classes=8):\n        super(CustomEfficientNet, self).__init__()\n\n        # Load EfficientNet-B0 pre-trained model\n        self.base_model = models.efficientnet_b1(pretrained=True)\n        \n        # Modify the final fully connected layer\n        in_features = self.base_model.classifier[1].in_features\n        self.base_model.classifier = nn.Sequential(\n            nn.Linear(in_features, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        return self.base_model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:53:34.066803Z","iopub.execute_input":"2025-02-26T09:53:34.067117Z","iopub.status.idle":"2025-02-26T09:53:34.072125Z","shell.execute_reply.started":"2025-02-26T09:53:34.067095Z","shell.execute_reply":"2025-02-26T09:53:34.071315Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = CustomEfficientNet(num_classes=8).to(device)\n\noptimizer = optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:53:39.697682Z","iopub.execute_input":"2025-02-26T09:53:39.698026Z","iopub.status.idle":"2025-02-26T09:53:40.663256Z","shell.execute_reply.started":"2025-02-26T09:53:39.698002Z","shell.execute_reply":"2025-02-26T09:53:40.662561Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b1_rwightman-bac287d4.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1_rwightman-bac287d4.pth\n100%|██████████| 30.1M/30.1M [00:00<00:00, 170MB/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# def train_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=25):\n#     best_val_acc = 0.0\n#     for epoch in range(num_epochs):\n#         model.train()  # Set to training mode\n#         running_loss, running_corrects = 0.0, 0\n\n#         # Training loop\n#         for inputs, labels in train_loader:\n#             inputs, labels = inputs.to(device), labels.to(device)\n#             optimizer.zero_grad()\n#             outputs = model(inputs)\n#             loss = criterion(outputs, labels)\n#             loss.backward()\n#             optimizer.step()\n\n#             _, preds = torch.max(outputs, 1)\n#             running_loss += loss.item() * inputs.size(0)\n#             running_corrects += torch.sum(preds == labels.data)\n\n#         epoch_loss = running_loss / len(train_loader.dataset)\n#         epoch_acc = running_corrects.double() / len(train_loader.dataset)\n#         print(f'Training - Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n\n#         # Validation loop\n#         model.eval()  # Set to evaluation mode\n#         val_corrects = 0\n#         with torch.no_grad():\n#             for inputs, labels in val_loader:\n#                 inputs, labels = inputs.to(device), labels.to(device)\n#                 outputs = model(inputs)\n#                 _, preds = torch.max(outputs, 1)\n#                 val_corrects += torch.sum(preds == labels.data)\n\n#         val_acc = val_corrects.double() / len(val_loader.dataset)\n#         print(f'Validation Accuracy: {val_acc:.4f}')\n\n#         # Save the best model based on validation accuracy\n#         if val_acc > best_val_acc:\n#             best_val_acc = val_acc\n#             torch.save(model.state_dict(), 'best_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T05:48:54.137734Z","iopub.execute_input":"2025-02-25T05:48:54.138061Z","iopub.status.idle":"2025-02-25T05:48:54.145075Z","shell.execute_reply.started":"2025-02-25T05:48:54.138036Z","shell.execute_reply":"2025-02-25T05:48:54.144368Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def train_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=25):\n    best_val_acc = 0.0\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"train_acc\": [],\n        \"val_acc\": []\n    }\n\n    for epoch in range(num_epochs):\n        model.train()  \n        running_loss, running_corrects = 0.0, 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, preds = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        train_epoch_loss = running_loss / len(train_loader.dataset)\n        train_epoch_acc = running_corrects.double() / len(train_loader.dataset)\n\n        \n        model.eval()  \n        val_loss, val_corrects = 0.0, 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                _, preds = torch.max(outputs, 1)\n                val_loss += loss.item() * inputs.size(0)\n                val_corrects += torch.sum(preds == labels.data)\n\n        val_epoch_loss = val_loss / len(val_loader.dataset)\n        val_epoch_acc = val_corrects.double() / len(val_loader.dataset)\n\n      \n        history[\"train_loss\"].append(train_epoch_loss)\n        history[\"val_loss\"].append(val_epoch_loss)\n        history[\"train_acc\"].append(train_epoch_acc.item())\n        history[\"val_acc\"].append(val_epoch_acc.item())\n\n        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_acc:.4f} - Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n\n        \n        if val_epoch_acc > best_val_acc:\n            best_val_acc = val_epoch_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:45:02.946098Z","iopub.execute_input":"2025-02-25T08:45:02.946458Z","iopub.status.idle":"2025-02-25T08:45:02.954629Z","shell.execute_reply.started":"2025-02-25T08:45:02.946408Z","shell.execute_reply":"2025-02-25T08:45:02.953740Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"train_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T05:49:04.648096Z","iopub.execute_input":"2025-02-25T05:49:04.648377Z","iopub.status.idle":"2025-02-25T08:17:38.881119Z","shell.execute_reply.started":"2025-02-25T05:49:04.648354Z","shell.execute_reply":"2025-02-25T08:17:38.880175Z"}},"outputs":[{"name":"stdout","text":"Training - Epoch 1/15, Loss: 0.9996, Accuracy: 0.6491\nValidation Accuracy: 0.6988\nTraining - Epoch 2/15, Loss: 0.8225, Accuracy: 0.7075\nValidation Accuracy: 0.7186\nTraining - Epoch 3/15, Loss: 0.7403, Accuracy: 0.7352\nValidation Accuracy: 0.7251\nTraining - Epoch 4/15, Loss: 0.6812, Accuracy: 0.7566\nValidation Accuracy: 0.7630\nTraining - Epoch 5/15, Loss: 0.6305, Accuracy: 0.7753\nValidation Accuracy: 0.7665\nTraining - Epoch 6/15, Loss: 0.5905, Accuracy: 0.7884\nValidation Accuracy: 0.7778\nTraining - Epoch 7/15, Loss: 0.5456, Accuracy: 0.8031\nValidation Accuracy: 0.7657\nTraining - Epoch 8/15, Loss: 0.5086, Accuracy: 0.8180\nValidation Accuracy: 0.7878\nTraining - Epoch 9/15, Loss: 0.4845, Accuracy: 0.8242\nValidation Accuracy: 0.7861\nTraining - Epoch 10/15, Loss: 0.4527, Accuracy: 0.8384\nValidation Accuracy: 0.7930\nTraining - Epoch 11/15, Loss: 0.4303, Accuracy: 0.8455\nValidation Accuracy: 0.8011\nTraining - Epoch 12/15, Loss: 0.4032, Accuracy: 0.8561\nValidation Accuracy: 0.8038\nTraining - Epoch 13/15, Loss: 0.3732, Accuracy: 0.8670\nValidation Accuracy: 0.8082\nTraining - Epoch 14/15, Loss: 0.3600, Accuracy: 0.8687\nValidation Accuracy: 0.8026\nTraining - Epoch 15/15, Loss: 0.3435, Accuracy: 0.8756\nValidation Accuracy: 0.8028\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"for param in model.base_model.features[6:].parameters():\n    param.requires_grad = True\n\n# Re-define optimizer for fine-tuning\noptimizer = optim.SGD(model.parameters(), lr=0.0001)\n\n# Fine-tune the model\nhistory = train_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:47:28.052694Z","iopub.execute_input":"2025-02-25T08:47:28.053037Z","iopub.status.idle":"2025-02-25T10:30:44.674762Z","shell.execute_reply.started":"2025-02-25T08:47:28.053012Z","shell.execute_reply":"2025-02-25T10:30:44.673979Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10 - Train Loss: 0.3167, Train Acc: 0.8878 - Val Loss: 0.6010, Val Acc: 0.8084\nEpoch 2/10 - Train Loss: 0.3137, Train Acc: 0.8877 - Val Loss: 0.5714, Val Acc: 0.8133\nEpoch 3/10 - Train Loss: 0.3068, Train Acc: 0.8875 - Val Loss: 0.5825, Val Acc: 0.8180\nEpoch 4/10 - Train Loss: 0.2998, Train Acc: 0.8923 - Val Loss: 0.5728, Val Acc: 0.8174\nEpoch 5/10 - Train Loss: 0.3043, Train Acc: 0.8908 - Val Loss: 0.5684, Val Acc: 0.8186\nEpoch 6/10 - Train Loss: 0.2988, Train Acc: 0.8918 - Val Loss: 0.5889, Val Acc: 0.8131\nEpoch 7/10 - Train Loss: 0.3020, Train Acc: 0.8938 - Val Loss: 0.5826, Val Acc: 0.8171\nEpoch 8/10 - Train Loss: 0.2947, Train Acc: 0.8935 - Val Loss: 0.5821, Val Acc: 0.8171\nEpoch 9/10 - Train Loss: 0.2991, Train Acc: 0.8942 - Val Loss: 0.5784, Val Acc: 0.8167\nEpoch 10/10 - Train Loss: 0.2988, Train Acc: 0.8898 - Val Loss: 0.5576, Val Acc: 0.8228\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history[\"train_loss\"], label=\"Train Loss\", marker='o')\nplt.plot(history[\"val_loss\"], label=\"Validation Loss\", marker='o')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Train & Validation Loss\")\nplt.legend()\n\n# Vẽ biểu đồ Train Accuracy và Validation Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history[\"train_acc\"], label=\"Train Accuracy\", marker='o')\nplt.plot(history[\"val_acc\"], label=\"Validation Accuracy\", marker='o')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Train & Validation Accuracy\")\nplt.legend()\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T10:43:13.570643Z","iopub.execute_input":"2025-02-25T10:43:13.571343Z","iopub.status.idle":"2025-02-25T10:43:13.575113Z","shell.execute_reply.started":"2025-02-25T10:43:13.571310Z","shell.execute_reply":"2025-02-25T10:43:13.574068Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/effbo_2502/pytorch/default/1/effb0_2502.pth', map_location=device))\n\n\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:54:08.399992Z","iopub.execute_input":"2025-02-26T09:54:08.400392Z","iopub.status.idle":"2025-02-26T09:54:09.094050Z","shell.execute_reply.started":"2025-02-26T09:54:08.400360Z","shell.execute_reply":"2025-02-26T09:54:09.093272Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-4deb0a6f41e6>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/effbo_2502/pytorch/default/1/effb0_2502.pth', map_location=device))\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"CustomEfficientNet(\n  (base_model): EfficientNet(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): SiLU(inplace=True)\n      )\n      (1): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (2): Conv2dNormActivation(\n              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (2): Conv2dNormActivation(\n              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n        )\n      )\n      (2): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n        )\n        (2): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n        )\n      )\n      (3): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n        )\n        (2): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n        )\n      )\n      (4): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n        )\n        (2): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n        )\n        (3): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n        )\n      )\n      (5): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n        )\n        (2): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n        )\n        (3): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n        )\n      )\n      (6): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n        )\n        (2): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n        )\n        (3): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n        )\n        (4): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n        )\n      )\n      (7): Sequential(\n        (0): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n        )\n        (1): MBConv(\n          (block): Sequential(\n            (0): Conv2dNormActivation(\n              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (1): Conv2dNormActivation(\n              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (2): SiLU(inplace=True)\n            )\n            (2): SqueezeExcitation(\n              (avgpool): AdaptiveAvgPool2d(output_size=1)\n              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n              (activation): SiLU(inplace=True)\n              (scale_activation): Sigmoid()\n            )\n            (3): Conv2dNormActivation(\n              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n        )\n      )\n      (8): Conv2dNormActivation(\n        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): SiLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=1)\n    (classifier): Sequential(\n      (0): Linear(in_features=1280, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Dropout(p=0.5, inplace=False)\n      (3): Linear(in_features=256, out_features=8, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def predict(model, test_loader):\n    model.eval()  \n    predictions = []\n    image_ids = []\n\n    with torch.no_grad(): \n        for images, img_id in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)  \n            \n            predictions.extend(preds.cpu().numpy()) \n            image_ids.extend(img_id)  \n\n    return image_ids, predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:54:17.745191Z","iopub.execute_input":"2025-02-26T09:54:17.745597Z","iopub.status.idle":"2025-02-26T09:54:17.750582Z","shell.execute_reply.started":"2025-02-26T09:54:17.745564Z","shell.execute_reply":"2025-02-26T09:54:17.749807Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"image_ids, preds = predict(model, test_loader)\nclass_mapping_inv = {v: k for k, v in train_dataset.class_mapping.items()}\npred_classes = [class_mapping_inv[p] for p in preds]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:54:20.385948Z","iopub.execute_input":"2025-02-26T09:54:20.386227Z","iopub.status.idle":"2025-02-26T09:58:06.460054Z","shell.execute_reply.started":"2025-02-26T09:54:20.386208Z","shell.execute_reply":"2025-02-26T09:58:06.459285Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"submission_df = pd.DataFrame({'image_id': image_ids, 'predicted_class': pred_classes})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:58:37.444328Z","iopub.execute_input":"2025-02-26T09:58:37.444668Z","iopub.status.idle":"2025-02-26T09:58:37.450031Z","shell.execute_reply.started":"2025-02-26T09:58:37.444645Z","shell.execute_reply":"2025-02-26T09:58:37.449325Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:58:39.896990Z","iopub.execute_input":"2025-02-26T09:58:39.897295Z","iopub.status.idle":"2025-02-26T09:58:39.956755Z","shell.execute_reply.started":"2025-02-26T09:58:39.897270Z","shell.execute_reply":"2025-02-26T09:58:39.955887Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"          image_id predicted_class\n0     ISIC_0034321             BKL\n1     ISIC_0034322              NV\n2     ISIC_0034323             BCC\n3     ISIC_0034324              NV\n4     ISIC_0034325              NV\n...            ...             ...\n6186  ISIC_0073226             BCC\n6187  ISIC_0073234              NV\n6188  ISIC_0073236             BCC\n6189  ISIC_0073243             BCC\n6190  ISIC_0073250              NV\n\n[6191 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>predicted_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0034321</td>\n      <td>BKL</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0034322</td>\n      <td>NV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0034323</td>\n      <td>BCC</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0034324</td>\n      <td>NV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0034325</td>\n      <td>NV</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6186</th>\n      <td>ISIC_0073226</td>\n      <td>BCC</td>\n    </tr>\n    <tr>\n      <th>6187</th>\n      <td>ISIC_0073234</td>\n      <td>NV</td>\n    </tr>\n    <tr>\n      <th>6188</th>\n      <td>ISIC_0073236</td>\n      <td>BCC</td>\n    </tr>\n    <tr>\n      <th>6189</th>\n      <td>ISIC_0073243</td>\n      <td>BCC</td>\n    </tr>\n    <tr>\n      <th>6190</th>\n      <td>ISIC_0073250</td>\n      <td>NV</td>\n    </tr>\n  </tbody>\n</table>\n<p>6191 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"test_gt = pd.read_csv('/kaggle/input/model-datatest-skincancer/test_gt_remove_unk.csv')\ntest_gt.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:58:43.216382Z","iopub.execute_input":"2025-02-26T09:58:43.216666Z","iopub.status.idle":"2025-02-26T09:58:43.242413Z","shell.execute_reply.started":"2025-02-26T09:58:43.216646Z","shell.execute_reply":"2025-02-26T09:58:43.241694Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"          image diagnosis\n0  ISIC_0034321        NV\n1  ISIC_0034322        NV\n2  ISIC_0034323       BCC\n3  ISIC_0034324        NV\n4  ISIC_0034325        NV","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0034321</td>\n      <td>NV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0034322</td>\n      <td>NV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0034323</td>\n      <td>BCC</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0034324</td>\n      <td>NV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0034325</td>\n      <td>NV</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"accuracy = (submission_df['predicted_class'] == test_gt['diagnosis']).mean()\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:58:45.637145Z","iopub.execute_input":"2025-02-26T09:58:45.637414Z","iopub.status.idle":"2025-02-26T09:58:45.650500Z","shell.execute_reply.started":"2025-02-26T09:58:45.637395Z","shell.execute_reply":"2025-02-26T09:58:45.649807Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.7068\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"test_gt.rename(columns={\"image\": \"image_id\", \"diagnosis\": \"true_class\"}, inplace=True)\n\ndf_merged = submission_df.merge(test_gt, on=\"image_id\")\n\nmisclassified_df = df_merged[df_merged['predicted_class'] != df_merged['true_class']]\nprint(len(misclassified_df))\ndisplay(misclassified_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:58:49.035261Z","iopub.execute_input":"2025-02-26T09:58:49.035570Z","iopub.status.idle":"2025-02-26T09:58:49.076852Z","shell.execute_reply.started":"2025-02-26T09:58:49.035545Z","shell.execute_reply":"2025-02-26T09:58:49.075949Z"}},"outputs":[{"name":"stdout","text":"1815\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        image_id predicted_class true_class\n0   ISIC_0034321             BKL         NV\n7   ISIC_0034329              NV        MEL\n21  ISIC_0034343              NV        MEL\n24  ISIC_0034346             SCC        BCC\n31  ISIC_0034354             BKL        MEL","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>predicted_class</th>\n      <th>true_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0034321</td>\n      <td>BKL</td>\n      <td>NV</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ISIC_0034329</td>\n      <td>NV</td>\n      <td>MEL</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>ISIC_0034343</td>\n      <td>NV</td>\n      <td>MEL</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>ISIC_0034346</td>\n      <td>SCC</td>\n      <td>BCC</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>ISIC_0034354</td>\n      <td>BKL</td>\n      <td>MEL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import torch.nn.functional as F\ndef predict_with_prob(model, test_loader):\n    model.eval()\n    predictions = []\n    image_ids = []\n    probabilities = []\n\n    with torch.no_grad():\n        for images, img_id in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            probs = F.softmax(outputs, dim=1)  # Convert logits thành xác suất\n            max_probs, preds = torch.max(probs, 1)  \n\n            predictions.extend(preds.cpu().numpy())\n            image_ids.extend(img_id)\n            probabilities.extend(max_probs.cpu().numpy())  # Lưu xác suất\n\n    return image_ids, predictions, probabilities\n\n\nimage_ids, preds, probs = predict_with_prob(model, test_loader)\n\n\nsubmission_df = pd.DataFrame({'image_id': image_ids, 'predicted_class': pred_classes, 'probability': probs})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:59:08.886125Z","iopub.execute_input":"2025-02-26T09:59:08.886418Z","iopub.status.idle":"2025-02-26T10:01:05.037865Z","shell.execute_reply.started":"2025-02-26T09:59:08.886398Z","shell.execute_reply":"2025-02-26T10:01:05.037165Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:01:53.403329Z","iopub.execute_input":"2025-02-26T10:01:53.403631Z","iopub.status.idle":"2025-02-26T10:01:53.416229Z","shell.execute_reply.started":"2025-02-26T10:01:53.403609Z","shell.execute_reply":"2025-02-26T10:01:53.415303Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"          image_id predicted_class  probability\n0     ISIC_0034321             BKL     0.732439\n1     ISIC_0034322              NV     0.999998\n2     ISIC_0034323             BCC     0.999567\n3     ISIC_0034324              NV     1.000000\n4     ISIC_0034325              NV     1.000000\n...            ...             ...          ...\n6186  ISIC_0073226             BCC     0.986773\n6187  ISIC_0073234              NV     0.918639\n6188  ISIC_0073236             BCC     0.961431\n6189  ISIC_0073243             BCC     0.625059\n6190  ISIC_0073250              NV     0.994056\n\n[6191 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>predicted_class</th>\n      <th>probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0034321</td>\n      <td>BKL</td>\n      <td>0.732439</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0034322</td>\n      <td>NV</td>\n      <td>0.999998</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0034323</td>\n      <td>BCC</td>\n      <td>0.999567</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0034324</td>\n      <td>NV</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0034325</td>\n      <td>NV</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6186</th>\n      <td>ISIC_0073226</td>\n      <td>BCC</td>\n      <td>0.986773</td>\n    </tr>\n    <tr>\n      <th>6187</th>\n      <td>ISIC_0073234</td>\n      <td>NV</td>\n      <td>0.918639</td>\n    </tr>\n    <tr>\n      <th>6188</th>\n      <td>ISIC_0073236</td>\n      <td>BCC</td>\n      <td>0.961431</td>\n    </tr>\n    <tr>\n      <th>6189</th>\n      <td>ISIC_0073243</td>\n      <td>BCC</td>\n      <td>0.625059</td>\n    </tr>\n    <tr>\n      <th>6190</th>\n      <td>ISIC_0073250</td>\n      <td>NV</td>\n      <td>0.994056</td>\n    </tr>\n  </tbody>\n</table>\n<p>6191 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"uncertain_df = submission_df[(submission_df['probability'] >= 0.45) & (submission_df['probability'] <= 0.55)]\nprint(len(uncertain_df))\ndisplay(uncertain_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:02:05.498935Z","iopub.execute_input":"2025-02-26T10:02:05.499227Z","iopub.status.idle":"2025-02-26T10:02:05.510937Z","shell.execute_reply.started":"2025-02-26T10:02:05.499208Z","shell.execute_reply":"2025-02-26T10:02:05.509994Z"}},"outputs":[{"name":"stdout","text":"383\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         image_id predicted_class  probability\n95   ISIC_0034422             BCC     0.456413\n104  ISIC_0034431              NV     0.537871\n131  ISIC_0034459             BKL     0.540407\n216  ISIC_0034547             BKL     0.508060\n242  ISIC_0034574             BCC     0.488606","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>predicted_class</th>\n      <th>probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>95</th>\n      <td>ISIC_0034422</td>\n      <td>BCC</td>\n      <td>0.456413</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>ISIC_0034431</td>\n      <td>NV</td>\n      <td>0.537871</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>ISIC_0034459</td>\n      <td>BKL</td>\n      <td>0.540407</td>\n    </tr>\n    <tr>\n      <th>216</th>\n      <td>ISIC_0034547</td>\n      <td>BKL</td>\n      <td>0.508060</td>\n    </tr>\n    <tr>\n      <th>242</th>\n      <td>ISIC_0034574</td>\n      <td>BCC</td>\n      <td>0.488606</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}